{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def resize_image(image_path, size):\n",
    "    \"\"\"resize image so that the largest edge is atmost size\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    if width <= size and height <= size:\n",
    "        return img\n",
    "\n",
    "    if width > height:\n",
    "        new_width = size\n",
    "        new_height = int(height * (size / width))\n",
    "    else:\n",
    "        new_height = size\n",
    "        new_width = int(width * (size / height))\n",
    "    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    img = resize_image(image_path, 512)\n",
    "    temp_name = \"temp.jpg\"\n",
    "    img.save(temp_name)\n",
    "    with open(temp_name, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def construct_mcq(options, correct_option):\n",
    "    correct_option_letter = None\n",
    "    i = \"a\"\n",
    "    mcq = \"\"\n",
    "\n",
    "    for option in options:\n",
    "        if option == correct_option:\n",
    "            correct_option_letter = i\n",
    "        mcq += f\"{i}. {option}\\n\"\n",
    "        i = chr(ord(i) + 1)\n",
    "\n",
    "    mcq = mcq[:-1]\n",
    "    return mcq, correct_option_letter\n",
    "\n",
    "\n",
    "def add_row(content: list[dict], data: dict, i: int, with_answer=False) -> list[dict]:\n",
    "    content.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Image \" + str(i) + \": \" + data[\"question\"] + \"\\n\" + data[\"mcq\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    content.append(\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/jpeg\",\n",
    "                \"data\": encode_image(data[\"image_path\"]),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if with_answer:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Reasoning: {}\".format(data[\"reasoning\"]),\n",
    "            }\n",
    "        )\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Answer {}: \".format(i) + data[\"correct_option_letter\"],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Answer {}: \".format(i),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOT_JSON = \"./illusionVQA/sofloc/fewshot_labels.json\"\n",
    "FEWSHOT_IMAGE_DIR = \"./illusionVQA/sofloc/FEW_SHOTS/\"\n",
    "EVAL_JSON = \"./illusionVQA/sofloc/eval_labels.json\"\n",
    "EVAL_IMAGE_DIR = \"./illusionVQA/sofloc/EVAL/\"\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "model_name = \"claude-3-5-sonnet-20240620\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(FEWSHOT_JSON) as f:\n",
    "    fewshot_dataset = json.load(f)\n",
    "\n",
    "for data in fewshot_dataset:\n",
    "    data[\"image_path\"] = FEWSHOT_IMAGE_DIR + data[\"image\"]\n",
    "    data[\"mcq\"], data[\"correct_option_letter\"] = construct_mcq(\n",
    "        data[\"options\"], data[\"answer\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EVAL_JSON) as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "random.shuffle(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = defaultdict(int)\n",
    "misc_cats = [\n",
    "    \"counting\",\n",
    "    \"repeating pattern\",\n",
    "    \"perspective\",\n",
    "    \"occlusion\",\n",
    "    \"angle constancy\",\n",
    "]\n",
    "\n",
    "for data in eval_dataset:\n",
    "    if data[\"image\"] not in os.listdir(EVAL_IMAGE_DIR):\n",
    "        print(data[\"image\"], \"not found\")\n",
    "        continue\n",
    "    data[\"image_path\"] = EVAL_IMAGE_DIR + data[\"image\"]\n",
    "    data[\"mcq\"], data[\"correct_option_letter\"] = construct_mcq(\n",
    "        data[\"options\"], data[\"answer\"]\n",
    "    )\n",
    "    \n",
    "    if data[\"category\"] in misc_cats:\n",
    "        data[\"category\"] = \"misc\"\n",
    "        \n",
    "    category_count[data[\"category\"]] += 1\n",
    "\n",
    "print(category_count)\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\"\"You'll be given an image, an instruction and some choices. You have to select the correct one. Reason about the choices in the context of the question and the image. End your answer with \"Answer\": {letter_of_correct_choice} without the curly brackets. Here are a few examples:\"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "i = 1\n",
    "for data in fewshot_dataset:\n",
    "    content = add_row(content, data, i, with_answer=True)\n",
    "    i += 1\n",
    "\n",
    "content.append(\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"Now you try it!\",\n",
    "    }\n",
    ")\n",
    "\n",
    "next_idx = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = []\n",
    "ypred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MAX_RETRIES = 2\n",
    "for data in tqdm(eval_dataset):\n",
    "    content_t = add_row(content.copy(), data, next_idx, with_answer=False)\n",
    "    retries = MAX_RETRIES\n",
    "    while retries:\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=model_name,\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content_t,\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            # print(message)\n",
    "            claude_ans = message.content[0].text.lower().strip()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            if retries == 0:\n",
    "                claude_ans = \"Claude could not answer this question.\"\n",
    "                print(\"retries exhausted\")\n",
    "                break\n",
    "\n",
    "    answer = data[\"correct_option_letter\"].strip()\n",
    "    ytrue.append(answer)\n",
    "    ypred.append(claude_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [x[-1] for x in ypred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ypred)):\n",
    "    if ypred[i] == \"Claude could not answer this question.\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ytrue), len(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "print(accuracy_score(ytrue, ypred))\n",
    "print(confusion_matrix(ytrue, ypred))\n",
    "print(classification_report(ytrue, ypred))\n",
    "\n",
    "print(Counter(ytrue))\n",
    "print(Counter(ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prettytable\n",
    "from collections import defaultdict\n",
    "\n",
    "table = prettytable.PrettyTable()\n",
    "table.field_names = [\"Category\", \"Total\", \"Wrong\", \"Accuracy\"]\n",
    "\n",
    "got_wrong_dict = defaultdict(int)\n",
    "total_wrong = 0\n",
    "\n",
    "for i in range(len(ypred)):\n",
    "    if ypred[i] != ytrue[i]:\n",
    "        got_wrong_dict[eval_dataset[i][\"category\"]] += 1\n",
    "        total_wrong += 1\n",
    "\n",
    "\n",
    "for k, v in got_wrong_dict.items():\n",
    "    table.add_row([k, category_count[k], v, 1 - (v / category_count[k])])\n",
    "\n",
    "table.add_row(\n",
    "    [\n",
    "        \"Total\",\n",
    "        len(eval_dataset),\n",
    "        total_wrong,\n",
    "        1 - (total_wrong / len(eval_dataset)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# sort by total\n",
    "table.sortby = \"Total\"\n",
    "table.reversesort = True\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_copy = eval_dataset.copy()\n",
    "\n",
    "print(len(eval_dataset_copy))\n",
    "for i, data in enumerate(eval_dataset_copy):\n",
    "    # map letter to option f\n",
    "    if \"BLOCK\" in ypred[i]:\n",
    "        data[\"vlm_answer\"] = \"BLOCK\"\n",
    "    else:\n",
    "        try:\n",
    "            data[\"vlm_answer\"] = data[\"options\"][ord(ypred[i]) - ord(\"a\")]\n",
    "        except Exception as e:\n",
    "            data[\"vlm_answer\"] = ypred[i]\n",
    "            print(ypred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "METRIC_SAVE_DIR = \"../../results_and_evaluation/closed_source/results/\"\n",
    "\n",
    "with open(METRIC_SAVE_DIR + \"illusionvqa_soft_loc_claude_with_reasoning.json\", \"w\") as f:\n",
    "    json.dump(eval_dataset_copy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
