{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "from together import Together\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def resize_image(image_path, size):\n",
    "    \"\"\"resize image so that the largest edge is atmost size\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    if width <= size and height <= size:\n",
    "        return img\n",
    "\n",
    "    if width > height:\n",
    "        new_width = size\n",
    "        new_height = int(height * (size / width))\n",
    "    else:\n",
    "        new_height = size\n",
    "        new_width = int(width * (size / height))\n",
    "    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    img = resize_image(image_path, 512)\n",
    "    temp_name = \"temp.jpg\"\n",
    "    img.save(temp_name)\n",
    "    with open(temp_name, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def construct_mcq(options, correct_option):\n",
    "    correct_option_letter = None\n",
    "    i = \"a\"\n",
    "    mcq = \"\"\n",
    "\n",
    "    for option in options:\n",
    "        if option == correct_option:\n",
    "            correct_option_letter = i\n",
    "        mcq += f\"{i}. {option}\\n\"\n",
    "        i = chr(ord(i) + 1)\n",
    "\n",
    "    mcq = mcq[:-1]\n",
    "    return mcq, correct_option_letter\n",
    "\n",
    "\n",
    "def add_row(content, data, i, with_answer=False):\n",
    "    encoded_image = encode_image(data[\"image_path\"])\n",
    "\n",
    "    content.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Image \" + str(i) + \": \" + data[\"question\"] + \"\\n\" + data[\"mcq\"],\n",
    "        }\n",
    "    )\n",
    "    content.append(\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{encoded_image}\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if with_answer:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Answer {}: \".format(i) + data[\"correct_option_letter\"],\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Answer {}: \".format(i),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOT_JSON = \"illusionVQA/comprehension/fewshot_labels.json\"\n",
    "FEWSHOT_IMAGE_DIR = \"illusionVQA/comprehension/FEW_SHOTS/\"\n",
    "EVAL_JSON = \"illusionVQA/comprehension/eval_labels.json\"\n",
    "EVAL_IMAGE_DIR = \"illusionVQA/comprehension/EVAL/\"\n",
    "\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(FEWSHOT_JSON) as f:\n",
    "    fewshot_dataset = json.load(f)\n",
    "\n",
    "\n",
    "for data in fewshot_dataset:\n",
    "    data[\"image_path\"] = FEWSHOT_IMAGE_DIR + data[\"image\"]\n",
    "    data[\"mcq\"], data[\"correct_option_letter\"] = construct_mcq(\n",
    "        data[\"options\"], data[\"answer\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EVAL_JSON) as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "random.shuffle(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'deceptive design': 37, 'impossible object': 134, 'counting': 11, 'color': 23, 'edited-scene': 21, 'size': 46, 'hidden': 45, 'real-scene': 64, 'angle illusion': 26, 'angle constancy': 2, 'perspective': 2, 'circle-spiral': 6, 'upside-down': 7, 'positive-negative space': 7, 'occlusion': 2, 'repeating pattern': 2})\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "category_count = defaultdict(int)\n",
    "\n",
    "for data in eval_dataset:\n",
    "    if data[\"image\"] not in os.listdir(EVAL_IMAGE_DIR):\n",
    "        print(data[\"image\"])\n",
    "        continue\n",
    "    data[\"image_path\"] = EVAL_IMAGE_DIR + data[\"image\"]\n",
    "    data[\"mcq\"], data[\"correct_option_letter\"] = construct_mcq(\n",
    "        data[\"options\"], data[\"answer\"]\n",
    "    )\n",
    "    category_count[data[\"category\"]] += 1\n",
    "\n",
    "print(category_count)\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "\n",
    "content.append(\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"Image 1\\n\",\n",
    "    }\n",
    ")\n",
    "content.append(\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": f\"https://gratisography.com/wp-content/uploads/2024/03/gratisography-funflower-1170x780.jpg\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "content.append(\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"Explain the two images above\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text', 'text': 'Image 1\\n'},\n",
       " {'type': 'image_url',\n",
       "  'image_url': {'url': 'https://gratisography.com/wp-content/uploads/2024/03/gratisography-funflower-1170x780.jpg'}},\n",
       " {'type': 'text', 'text': 'Explain the two images above'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.7,\n",
    "    top_k=50,\n",
    "    stop=[\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a sunflower with sunglasses placed on its center, creating a playful and whimsical scene. The sunflower is positioned centrally in the image, with its bright yellow petals and dark brown center standing out against the orange background. The sunglasses add a touch of humor and personality to the flower, making it appear as if it's ready to take on the world with its stylish and cool demeanor.\n",
      "\n",
      "The overall effect of the image is one of joy and playfulness, inviting the viewer to smile and appreciate the simple pleasures in life. The use of bright colors and bold shapes creates a lively and energetic atmosphere, making the image feel like a celebration of summer and all its delights.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"You'll be given an image, an instruction and some options. You have to select the correct one. Do not explain your reasoning. Answer with only the letter which corresponds to the correct option. Do not repeat the entire answer. Do not output anything other than the correct letter. For example, if the correct option is 'a', you should only output 'a'.\",\n",
    "    }\n",
    "]\n",
    "next_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content = [\n",
    "#     {\n",
    "#         \"type\": \"text\",\n",
    "#         \"text\": \"You'll be given an image, an instruction and some choices. You have to select the correct one. Do not explain your reasoning. Answer with the option's letter from the given choices directly. Here are a few examples:\",\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "\n",
    "# i = 1\n",
    "# for data in fewshot_dataset:\n",
    "#     content = add_row(content, data, i, with_answer=True)\n",
    "#     i += 1\n",
    "\n",
    "# content.append({\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": \"Now you try it!\",\n",
    "#                 })\n",
    "\n",
    "# next_idx = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(output: str):\n",
    "    if output.startswith(\"Answer\"):\n",
    "        return output.split(\": \")[1][0].lower()\n",
    "    else:\n",
    "        return output[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = []\n",
    "ypred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [10:09<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MAX_RETRIES = 2\n",
    "for data in tqdm(eval_dataset):\n",
    "    content_t = add_row(content.copy(), data, next_idx, with_answer=False)\n",
    "    retries = MAX_RETRIES\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content_t,\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=512,\n",
    "                temperature=0.7,\n",
    "                top_p=0.7,\n",
    "                top_k=50,\n",
    "                stop=[\"<|eot_id|>\", \"<|eom_id|>\"],\n",
    "            )\n",
    "            intern_ans = parse_output(response.choices[0].message.content.strip())\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            retries -= 1\n",
    "            time.sleep(30)\n",
    "            if retries == 0:\n",
    "                intern_ans = \"GPT4 could not answer this question.\"\n",
    "                print(\"retries exhausted\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "    answer = data[\"correct_option_letter\"].strip()\n",
    "    ytrue.append(answer)\n",
    "    ypred.append(intern_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ypred)):\n",
    "    if ypred[i] == \"Llama 3.2 could not answer this question.\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 435)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ytrue), len(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4091954022988506\n",
      "[[62 22 18 11  1  0  0  1  1]\n",
      " [34 53 11  7  1  0  0  1  2]\n",
      " [34 20 35  8  2  0  1  3  6]\n",
      " [18 18 13 26  2  1  0  3  5]\n",
      " [ 2  2  2  2  2  0  0  0  1]\n",
      " [ 1  1  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       0.41      0.53      0.46       116\n",
      "           b       0.46      0.49      0.47       109\n",
      "           c       0.44      0.32      0.37       109\n",
      "           d       0.48      0.30      0.37        86\n",
      "           e       0.22      0.18      0.20        11\n",
      "           f       0.00      0.00      0.00         4\n",
      "           i       0.00      0.00      0.00         0\n",
      "           t       0.00      0.00      0.00         0\n",
      "           w       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.41       435\n",
      "   macro avg       0.22      0.20      0.21       435\n",
      "weighted avg       0.43      0.41      0.41       435\n",
      "\n",
      "Counter({'a': 116, 'c': 109, 'b': 109, 'd': 86, 'e': 11, 'f': 4})\n",
      "Counter({'a': 151, 'b': 116, 'c': 80, 'd': 54, 'w': 15, 'e': 9, 't': 8, 'i': 1, 'f': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salkhon/miniconda3/envs/molecule/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/salkhon/miniconda3/envs/molecule/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/salkhon/miniconda3/envs/molecule/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "print(accuracy_score(ytrue, ypred))\n",
    "print(confusion_matrix(ytrue, ypred))\n",
    "print(classification_report(ytrue, ypred))\n",
    "\n",
    "print(Counter(ytrue))\n",
    "print(Counter(ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------+-------+---------------------+\n",
      "|         Category        | Total | Wrong |       Accuracy      |\n",
      "+-------------------------+-------+-------+---------------------+\n",
      "|    impossible object    |  134  |   74  |  0.4477611940298507 |\n",
      "|        real-scene       |   64  |   34  |       0.46875       |\n",
      "|           size          |   46  |   33  | 0.28260869565217395 |\n",
      "|          hidden         |   45  |   27  |         0.4         |\n",
      "|     deceptive design    |   37  |   27  |  0.2702702702702703 |\n",
      "|      angle illusion     |   26  |   15  | 0.42307692307692313 |\n",
      "|          color          |   23  |   16  | 0.30434782608695654 |\n",
      "|       edited-scene      |   21  |   12  |  0.4285714285714286 |\n",
      "|         counting        |   11  |   7   | 0.36363636363636365 |\n",
      "|       upside-down       |   7   |   2   |  0.7142857142857143 |\n",
      "| positive-negative space |   7   |   4   |  0.4285714285714286 |\n",
      "|      circle-spiral      |   6   |   3   |         0.5         |\n",
      "|    repeating pattern    |   2   |   1   |         0.5         |\n",
      "|       perspective       |   2   |   0   |         1.0         |\n",
      "|        occlusion        |   2   |   0   |         1.0         |\n",
      "|     angle constancy     |   2   |   2   |         0.0         |\n",
      "+-------------------------+-------+-------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import prettytable\n",
    "from collections import defaultdict\n",
    "\n",
    "table = prettytable.PrettyTable()\n",
    "table.field_names = [\"Category\", \"Total\", \"Wrong\", \"Accuracy\"]\n",
    "\n",
    "got_wrong_dict = defaultdict(int)\n",
    "\n",
    "for i in range(len(ypred)):\n",
    "    if ypred[i] != ytrue[i]:\n",
    "        got_wrong_dict[eval_dataset[i][\"category\"]] += 1\n",
    "    else:\n",
    "        got_wrong_dict[eval_dataset[i][\"category\"]] += 0\n",
    "\n",
    "\n",
    "for k, v in got_wrong_dict.items():\n",
    "    table.add_row([k, category_count[k], v, 1 - (v / category_count[k])])\n",
    "\n",
    "\n",
    "# sort by total\n",
    "table.sortby = \"Total\"\n",
    "table.reversesort = True\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    }
   ],
   "source": [
    "METRIC_SAVE_DIR = \"../../result_jsons/\"\n",
    "\n",
    "eval_dataset_copy = eval_dataset.copy()\n",
    "\n",
    "print(len(eval_dataset_copy))\n",
    "for i, data in enumerate(eval_dataset_copy):\n",
    "    # map letter to option f\n",
    "    if \"BLOCK\" in ypred[i]:\n",
    "        data[\"vlm_answer\"] = \"BLOCK\"\n",
    "    else:\n",
    "        data[\"vlm_answer\"] = ypred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(METRIC_SAVE_DIR + \"llama3.2_11b_comprehension_0shot.json\", \"w\") as f:\n",
    "    json.dump(eval_dataset_copy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
